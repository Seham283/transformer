{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample tokenized input (7 words)\n",
        "tokens = [\"What\", \"are\", \"the\", \"symptoms\", \"of\", \"diabetes\", \"?\"]\n",
        "\n",
        "# Embedding matrix (each row is a token's embedding vector of size 4)\n",
        "E = np.array([\n",
        "    [1.0, 0.2, 0.5, 0.7],  # \"What\"\n",
        "    [0.6, 0.9, 0.4, 0.7],  # \"are\"\n",
        "    [0.4, 0.3, 1.1, 0.6],  # \"the\"\n",
        "    [1.1, 1.0, 0.7, 0.8],  # \"symptoms\"\n",
        "    [0.7, 0.8, 0.6, 0.5],  # \"of\"\n",
        "    [1.5, 1.2, 1.4, 0.9],  # \"diabetes\"\n",
        "    [0.8, 1.0, 1.3, 1.6]   # \"?\"\n",
        "])\n",
        "\n",
        "# Positional encoding\n",
        "pos = np.arange(7)[:, np.newaxis]  # Positions: [0, 1, 2, ..., 6]\n",
        "div_term = np.power(10000, 2 * np.arange(2) / 4)\n",
        "pe = np.zeros((7, 4))\n",
        "pe[:, 0::2] = np.sin(pos / div_term)\n",
        "pe[:, 1::2] = np.cos(pos / div_term)\n",
        "\n",
        "# Input representation after adding positional encoding\n",
        "X = E + pe\n",
        "\n",
        "# Query, Key, and Value weight matrices\n",
        "W_q = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7, 0.8],\n",
        "    [0.9, 1.0, 1.1, 1.2],\n",
        "    [1.3, 1.4, 1.5, 1.6]\n",
        "])\n",
        "\n",
        "W_k = np.array([\n",
        "    [0.2, 0.3, 0.4, 0.5],\n",
        "    [0.6, 0.7, 0.8, 0.9],\n",
        "    [1.0, 1.1, 1.2, 1.3],\n",
        "    [1.4, 1.5, 1.6, 1.7]\n",
        "])\n",
        "\n",
        "W_v = np.array([\n",
        "    [0.3, 0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9, 1.0],\n",
        "    [1.1, 1.2, 1.3, 1.4],\n",
        "    [1.5, 1.6, 1.7, 1.8]\n",
        "])\n",
        "\n",
        "# Compute Queries (Q), Keys (K), and Values (V)\n",
        "Q = X @ W_q  # Shape: (7, 4)\n",
        "K = X @ W_k  # Shape: (7, 4)\n",
        "V = X @ W_v  # Shape: (7, 4)\n",
        "\n",
        "# Compute Attention Scores (Softmax(QK^T / sqrt(d_k)))\n",
        "d_k = 4  # Scaling factor (dimension of key vectors)\n",
        "scores = Q @ K.T / np.sqrt(d_k)  # Raw attention scores (before softmax)\n",
        "attention_scores = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax normalization\n",
        "\n",
        "# Compute final output by multiplying attention scores with V\n",
        "output = attention_scores @ V\n",
        "\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Print attention scores with corresponding tokens\n",
        "print(\"\\nAttention Scores (How much each token attends to others):\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{token}: {attention_scores[i]}\")\n",
        "\n",
        "# Print final output (contextualized embeddings)\n",
        "print(\"\\nFinal Output (Transformed embeddings after self-attention):\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{token}: {output[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4KB0kej8EKV",
        "outputId": "759773b7-9f84-4928-fb32-63a4fb3d7204"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['What', 'are', 'the', 'symptoms', 'of', 'diabetes', '?']\n",
            "\n",
            "Attention Scores (How much each token attends to others):\n",
            "What: [1.52530715e-10 1.04420713e-09 1.09310055e-11 6.22907524e-12\n",
            " 3.19294245e-15 2.00522236e-05 9.99979947e-01]\n",
            "are: [5.68112283e-11 4.24813834e-10 3.61933423e-12 2.01077412e-12\n",
            " 7.33775331e-16 1.24883961e-05 9.99987511e-01]\n",
            "the: [7.12635500e-10 4.27032529e-09 6.12001752e-11 3.62527436e-11\n",
            " 3.12817775e-14 4.19508673e-05 9.99958044e-01]\n",
            "symptoms: [9.73978239e-10 5.67894679e-09 8.67864132e-11 5.18178843e-11\n",
            " 4.97265333e-14 4.87238192e-05 9.99951269e-01]\n",
            "of: [7.48874940e-08 2.99489478e-07 1.11215922e-08 7.40884345e-09\n",
            " 3.09542754e-11 3.89809591e-04 9.99609797e-01]\n",
            "diabetes: [7.67439809e-14 9.97370805e-13 2.27571079e-15 1.07465795e-15\n",
            " 4.36869080e-20 5.29943539e-07 9.99999470e-01]\n",
            "?: [7.97000379e-17 1.85326524e-15 1.06258755e-18 4.23184540e-19\n",
            " 1.73059012e-24 1.98005976e-08 9.99999980e-01]\n",
            "\n",
            "Final Output (Transformed embeddings after self-attention):\n",
            "What: [6.92153015 7.56541994 8.20930974 8.85319954]\n",
            "are: [6.92153981 7.56543042 8.20932102 8.85321162]\n",
            "the: [6.92150216 7.56538962 8.20927709 8.85316455]\n",
            "symptoms: [6.9214935  7.56538024 8.20926699 8.85315373]\n",
            "of: [6.92105676 7.5649071  8.20875744 8.85260777]\n",
            "diabetes: [6.92155509 7.56544697 8.20933885 8.85323072]\n",
            "?: [6.92155574 7.56544767 8.20933961 8.85323154]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sample tokenized input (7 words)\n",
        "tokens = [\"What\", \"are\", \"the\", \"symptoms\", \"of\", \"diabetes\", \"?\"]\n",
        "\n",
        "# Embedding matrix (each row is a token's embedding vector of size 4)\n",
        "E = torch.tensor([\n",
        "    [1.0, 0.2, 0.5, 0.7],  # \"What\"\n",
        "    [0.6, 0.9, 0.4, 0.7],  # \"are\"\n",
        "    [0.4, 0.3, 1.1, 0.6],  # \"the\"\n",
        "    [1.1, 1.0, 0.7, 0.8],  # \"symptoms\"\n",
        "    [0.7, 0.8, 0.6, 0.5],  # \"of\"\n",
        "    [1.5, 1.2, 1.4, 0.9],  # \"diabetes\"\n",
        "    [0.8, 1.0, 1.3, 1.6]   # \"?\"\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Positional encoding\n",
        "pos = torch.arange(7).unsqueeze(1)  # Positions: [0, 1, 2, ..., 6]\n",
        "div_term = torch.pow(10000, 2 * torch.arange(2, dtype=torch.float32) / 4)\n",
        "pe = torch.zeros((7, 4))\n",
        "pe[:, 0::2] = torch.sin(pos / div_term)\n",
        "pe[:, 1::2] = torch.cos(pos / div_term)\n",
        "\n",
        "# Input representation after adding positional encoding\n",
        "X = E + pe\n",
        "\n",
        "# Query, Key, and Value weight matrices\n",
        "W_q = torch.tensor([\n",
        "    [0.1, 0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7, 0.8],\n",
        "    [0.9, 1.0, 1.1, 1.2],\n",
        "    [1.3, 1.4, 1.5, 1.6]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "W_k = torch.tensor([\n",
        "    [0.2, 0.3, 0.4, 0.5],\n",
        "    [0.6, 0.7, 0.8, 0.9],\n",
        "    [1.0, 1.1, 1.2, 1.3],\n",
        "    [1.4, 1.5, 1.6, 1.7]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "W_v = torch.tensor([\n",
        "    [0.3, 0.4, 0.5, 0.6],\n",
        "    [0.7, 0.8, 0.9, 1.0],\n",
        "    [1.1, 1.2, 1.3, 1.4],\n",
        "    [1.5, 1.6, 1.7, 1.8]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Compute Queries (Q), Keys (K), and Values (V)\n",
        "Q = X @ W_q  # Shape: (7, 4)\n",
        "K = X @ W_k  # Shape: (7, 4)\n",
        "V = X @ W_v  # Shape: (7, 4)\n",
        "\n",
        "# Compute Attention Scores (Softmax(QK^T / sqrt(d_k)))\n",
        "d_k = 4  # Scaling factor (dimension of key vectors)\n",
        "scores = (Q @ K.T) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))  # Raw attention scores\n",
        "attention_scores = F.softmax(scores, dim=-1)  # Apply softmax normalization\n",
        "\n",
        "# Compute final output by multiplying attention scores with V\n",
        "output = attention_scores @ V\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Print attention scores with corresponding tokens\n",
        "print(\"\\nAttention Scores (How much each token attends to others):\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{token}: {attention_scores[i].tolist()}\")\n",
        "\n",
        "# Print final output (contextualized embeddings)\n",
        "print(\"\\nFinal Output (Transformed embeddings after self-attention):\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{token}: {output[i].tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj8ppad56ksH",
        "outputId": "5eddb458-b67d-4d77-d26b-ca35aa9c7df6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['What', 'are', 'the', 'symptoms', 'of', 'diabetes', '?']\n",
            "\n",
            "Attention Scores (How much each token attends to others):\n",
            "What: [1.5253090457356677e-10, 1.0442061659787782e-09, 1.0930993089852681e-11, 6.2290758488792175e-12, 3.1929368579707105e-15, 2.005222995649092e-05, 0.9999799728393555]\n",
            "are: [5.681146525438052e-11, 4.248123797179204e-10, 3.6193326981293072e-12, 2.010776744762466e-12, 7.33775430723099e-16, 1.2488432730606291e-05, 0.9999874830245972]\n",
            "the: [7.126363388287871e-10, 4.270324538424575e-09, 6.120032952638965e-11, 3.6252733454089636e-11, 3.128176022252141e-14, 4.195092697045766e-05, 0.9999580383300781]\n",
            "symptoms: [9.739811179798608e-10, 5.678931103147988e-09, 8.678639751291684e-11, 5.1817782703578175e-11, 4.972644475007004e-14, 4.872378121945076e-05, 0.9999512434005737]\n",
            "of: [7.488761610829897e-08, 2.9948901669740735e-07, 1.112158543037367e-08, 7.408841096179231e-09, 3.0954263458005116e-11, 0.0003898085851687938, 0.9996097683906555]\n",
            "diabetes: [7.67443969701756e-14, 9.973686373315749e-13, 2.275709443344499e-15, 1.0746573817193694e-15, 4.368682350012644e-20, 5.299446570461441e-07, 0.9999995231628418]\n",
            "?: [7.97001490386847e-17, 1.853254725516732e-15, 1.062585978525539e-18, 4.2318376607514486e-19, 1.7305878683014302e-24, 1.9800694417426712e-08, 1.0]\n",
            "\n",
            "Final Output (Transformed embeddings after self-attention):\n",
            "What: [6.921530246734619, 7.565420627593994, 8.209309577941895, 8.85319995880127]\n",
            "are: [6.921539306640625, 7.565430641174316, 8.209320068359375, 8.853211402893066]\n",
            "the: [6.921501636505127, 7.565390110015869, 8.20927619934082, 8.853164672851562]\n",
            "symptoms: [6.921493053436279, 7.565380573272705, 8.209266662597656, 8.853153228759766]\n",
            "of: [6.921056270599365, 7.564907550811768, 8.208756446838379, 8.852607727050781]\n",
            "diabetes: [6.921555042266846, 7.565447807312012, 8.209339141845703, 8.853231430053711]\n",
            "?: [6.921555519104004, 7.56544828414917, 8.209339141845703, 8.853231430053711]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jm7XfpjP-vRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}